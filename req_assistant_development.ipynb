{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Story Driven Requirements with GPT-4\n",
    "This notebook explores the use of utilizing GPT 4 on a domain-specific set of requirements. This model will be used downstream to manage requirements changes provided by user stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "IEEE_830 = \"inputs/IEEE 830-1998.pdf\"\n",
    "BRIDGE_INSPECTION = \"inputs/a-state-of-the-art-review-of-bridge-inspection-planning-current-situation-and-future-needs.pdf\"\n",
    "EARS_SYNTAX = \"inputs/EARS_syntax.md\"\n",
    "EDIT_MD_PATH = \"inputs/edited_srs.md\"\n",
    "USER_STORIES_PATH = \"inputs/user_story_set.json\"\n",
    "RAG_DOCUMENTS = [EDIT_MD_PATH, IEEE_830, EARS_SYNTAX, BRIDGE_INSPECTION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the instructions from the prompt file\n",
    "SYSTEM_INSTRUCTIONS = open(\"prompts/system_instructions.txt\", \"r\").read()\n",
    "VERIFIABILITY_INSTRUCTIONS = open(\"prompts/verifiable_instructions.txt\", \"r\").read()\n",
    "UNAMBIGUOUS_INSTRUCTIONS = open(\"prompts/unambiguous_instructions.txt\", \"r\").read()\n",
    "SYNTAX_INSTRUCTIONS = open(\"prompts/syntax_instructions.txt\", \"r\").read()\n",
    "CONSISTENCY_INSTRUCTIONS = open(\"prompts/consistency_instructions.txt\", \"r\").read()\n",
    "EVALUATION_INSTRUCTIONS = open(\"prompts/evaluation_instructions.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI api key\n",
    "import json\n",
    "\n",
    "config_data = json.load(open(\"config.json\"))\n",
    "openai_api_key = config_data[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OpenAI Client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user story and save location (update for each run to chaneg the loaded user story)\n",
    "import json\n",
    "\n",
    "CURRENT_STORY = 0\n",
    "USER_STORIES = json.load(open(USER_STORIES_PATH))\n",
    "USER_STORY = USER_STORIES[\"user-stories\"][CURRENT_STORY][\"user-story\"]\n",
    "OUTPUT_FILE_PREFIX = USER_STORIES[\"user-stories\"][CURRENT_STORY][\"output-location\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Virtual Assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new assistant with access to the existing system requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Virtual Assistant Agents\n",
    "- ```Story2Req```: converts user stories into a use case and a set of comprehensive functional requirements and nonfunctional requirements.\n",
    "- ```BetterReqFinder```: Compares two sets of requirements for the same user story and chooses the better one\n",
    "- ```UnambiguousReqGen```: Rewrites a given requirement statement into an unambiguous requirement\n",
    "- ```VerifiableReqGen```: Rewrites a given requirement statement to be verifiable\n",
    "- ```SyntaxReqGen```: Rewrites a given requirement statement to be formatted into the EARS requirement syntax\n",
    "- ```ConsistencyReqGen```: Rewrites a given set of requirements to use consistent terminology and sentence flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Story2Req\n",
    "def create_story2req_assistant():\n",
    "    assistant = client.beta.assistants.create(\n",
    "        name=\"User-Story Driven Requirements Generator\",\n",
    "        instructions=SYSTEM_INSTRUCTIONS,\n",
    "        model=MODEL_NAME,\n",
    "        tools=[{\"type\": \"file_search\"}]\n",
    "    )\n",
    "    return assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VerifiableReqGen\n",
    "def get_verifiable_requirement(requirement: str, user_story: str):\n",
    "    prompt = VERIFIABILITY_INSTRUCTIONS.format(\n",
    "        requirement=requirement,\n",
    "        user_story=user_story\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnambiguousReqGen\n",
    "def get_unambiguous_requirement(requirement: str, user_story: str):\n",
    "    prompt = UNAMBIGUOUS_INSTRUCTIONS.format(\n",
    "        requirement=requirement,\n",
    "        user_story=user_story\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SyntaxReqGen\n",
    "def get_formatted_requirements(requirement: str):\n",
    "    prompt = SYNTAX_INSTRUCTIONS.format(\n",
    "        requirement=requirement\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConsistencyReqGen\n",
    "def get_consistent_requirements(requirement_artifacts: str, user_story: str):\n",
    "    prompt = CONSISTENCY_INSTRUCTIONS.format(\n",
    "        requirement_artifacts=requirement_artifacts,\n",
    "        user_story=user_story\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0,\n",
    "        max_tokens=1500,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BetterReqFinder\n",
    "def get_better_requirement_set(requirement_1: str, requirement_2: str, user_story: str):\n",
    "    prompt = EVALUATION_INSTRUCTIONS.format(\n",
    "        requirement_1=requirement_1,\n",
    "        requirement_2=requirement_2,\n",
    "        user_story=user_story\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "\n",
    "    if (response.choices[0].message.content) == \"1\":\n",
    "        return requirement_1\n",
    "    else:\n",
    "        return requirement_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supporting Functions for Virtual Assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the set of requiremnets into a digestible string of text for the evaluator\n",
    "def get_requirement_set_string(req_json):\n",
    "    requirement_set = \"General Use Case:\"\n",
    "    for uc in req_json[\"use-cases\"]:\n",
    "        requirement_set += f\"\\n{uc['description']}\"\n",
    "        requirement_set += f\"\\n{uc['success-end-condition']}\"\n",
    "\n",
    "    ifr = 1\n",
    "    requirement_set += f\"\\n\\nFunctional Requirements:\"\n",
    "    for fr in req_json[\"functional-requirements\"]:\n",
    "        requirement_set += f\"\\n{ifr}. {fr['description']}\"\n",
    "        ifr += 1\n",
    "\n",
    "    infr = 1\n",
    "    requirement_set += f\"\\n\\nNon-Functional Requirements:\"\n",
    "    for nfr in req_json[\"non-functional-requirements\"]:\n",
    "        requirement_set += f\"\\n{infr}. {nfr['description']}\"\n",
    "        infr += 1\n",
    "    \n",
    "    return requirement_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the file and add them to a Vector Store. Then, update the assistant with the new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inject the Story2Req assistant with domain knowledge and necessary context information\n",
    "def knowledge_injection(vector_name, file_paths_list, assistant):\n",
    "    # Create a vector store\n",
    "    vector_store = client.beta.vector_stores.create(name=vector_name)\n",
    "    \n",
    "    # Ready the files for upload to OpenAI\n",
    "    file_paths = file_paths_list\n",
    "    file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "    \n",
    "    # Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "    # and poll the status of the file batch for completion.\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id, files=file_streams\n",
    "    )\n",
    "    \n",
    "    # You can print the status and the file counts of the batch to see the result of this operation.\n",
    "    print(file_batch.status)\n",
    "    print(file_batch.file_counts)\n",
    "    domain_assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "    return domain_assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a thread and attach the file to the message\n",
    "def create_thread(user_story):\n",
    "    # Upload the user provided file to OpenAI\n",
    "    message_file = client.files.create(\n",
    "    file=open(EDIT_MD_PATH, \"rb\"), purpose=\"assistants\"\n",
    "    )\n",
    "    \n",
    "    thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_story,\n",
    "        # Attach the new file to the message.\n",
    "        \"attachments\": [\n",
    "            { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "        ],\n",
    "        }\n",
    "    ]\n",
    "    )\n",
    "    # The thread now has a vector store with that file in its tool resources.\n",
    "    # print(thread.tool_resources.file_search)\n",
    "    return(thread)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a run and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "def generate_requirements(thread, assistant):\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id, assistant_id=assistant.id\n",
    "    )\n",
    "\n",
    "    messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "    message_content = messages[0].content[0].text\n",
    "    annotations = message_content.annotations\n",
    "    citations = []\n",
    "    for index, annotation in enumerate(annotations):\n",
    "        message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "        if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "            cited_file = client.files.retrieve(file_citation.file_id)\n",
    "            citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "            \n",
    "    return message_content.value\n",
    "\n",
    "# print(message_content.value)\n",
    "# print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=4, failed=0, in_progress=0, total=4)\n"
     ]
    }
   ],
   "source": [
    "# Create and configure the requirements story2req assistant\n",
    "story2req = create_story2req_assistant()\n",
    "story2req = knowledge_injection(\n",
    "    vector_name=\"Software Requirements and Domain Knowledge\",\n",
    "    file_paths_list=RAG_DOCUMENTS,\n",
    "    assistant=story2req\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diff_match_patch as dmp_module\n",
    "\n",
    "def difference(string1, string2):\n",
    "    dmp = dmp_module.diff_match_patch()\n",
    "    diff = dmp.diff_main(string1, string2)\n",
    "    dmp.diff_cleanupSemantic(diff)\n",
    "    temp = []\n",
    "    for item in diff:\n",
    "        if item[0] != 0:\n",
    "            temp.append(item)\n",
    "\n",
    "    if len(temp) == 0:\n",
    "        output = \"No changes detected\"\n",
    "    else:\n",
    "        output = \"\\n\".join(str(element) for element in temp)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate High-Quality Requirements from User Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the virtual assistant for user story translation to requirements and choose the best requirement set out of 5 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 0\n",
      "Completed iteration 1\n",
      "Completed iteration 2\n",
      "Completed iteration 3\n",
      "Completed iteration 4\n"
     ]
    }
   ],
   "source": [
    "# Generate requirements artifacts from the same user story multiple times\n",
    "import re\n",
    "\n",
    "# Just grab the json output (sometimes the model likes to provide reasoning before and after)\n",
    "delim_start = \"```json\"\n",
    "delim_end = \"```\"\n",
    "\n",
    "previous_requirements = \"\"\n",
    "best_requirement_set = \"\"\n",
    "better_requirements_log = \"\"\n",
    "for i in range(5):\n",
    "    better_requirements_log += f\"\\n{'-'*25}\"\n",
    "    # Create a thread using the original SRS and the chosen user story\n",
    "    thread = create_thread(USER_STORY)\n",
    "\n",
    "    # Run the thread to generate the requirements artifacts\n",
    "    response = generate_requirements(thread, story2req)\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Convert to JSON object\n",
    "        req_string = \"\".join(response.split(delim_start)[1].split(delim_end)[0])\n",
    "        req_json = json.loads(req_string)\n",
    "        requirement_set = get_requirement_set_string(req_json)\n",
    "        better_requirements_log += f\"\\nIteration {i+1}:\\n{req_string}\"\n",
    "        better_requirements_log += f\"\\n===\"\n",
    "\n",
    "        # Determine which is the best requirement set so far (winner used downstream)\n",
    "        if (previous_requirements == \"\"):\n",
    "            best_requirement_set = requirement_set\n",
    "            best_requirement_json = req_json\n",
    "        else:\n",
    "            best_requirement_set = get_better_requirement_set(best_requirement_set, requirement_set, USER_STORY)\n",
    "            if (best_requirement_set == requirement_set):\n",
    "                better_requirements_log += f\"\\nNew best found!\"\n",
    "                best_requirement_json = req_json\n",
    "            else:\n",
    "                better_requirements_log += f\"\\nPrevious set was better...\"\n",
    "        \n",
    "    except:\n",
    "        better_requirements_log += f\"\\nERROR: Failed to convert to JSON and evaluate\"\n",
    "        better_requirements_log += f\"\\n--\"\n",
    "        better_requirements_log += f\"\\nRESPONSE\"\n",
    "        better_requirements_log += f\"\\n{response}\"\n",
    "    \n",
    "    print(f\"Completed iteration {i}\")\n",
    "\n",
    "    previous_requirements = requirement_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate and optimize the requirement artifacts [ReqValidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the functional requirements for unambiguity\n",
    "functional_improvement_log = \"\"\n",
    "for fr in best_requirement_json['functional-requirements']:\n",
    "    unambiguous_fr = get_unambiguous_requirement(fr[\"description\"], USER_STORY)\n",
    "    formatted_fr = get_formatted_requirements(unambiguous_fr)\n",
    "    functional_improvement_log += f\"\\n- Original: {fr['description']}\\n- Unambiguous: {unambiguous_fr}\\n- Formatted: {formatted_fr}\\n---\"\n",
    "    fr[\"description\"] = formatted_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the non-functional requirements for verifiability\n",
    "nonfunctional_improvement_log = \"\"\n",
    "for nfr in best_requirement_json['non-functional-requirements']:\n",
    "    verifiable_nfr = get_verifiable_requirement(nfr[\"description\"], USER_STORY)\n",
    "    unambiguous_nfr = get_unambiguous_requirement(verifiable_nfr, USER_STORY)\n",
    "    formatted_nfr = get_formatted_requirements(unambiguous_nfr)\n",
    "    nonfunctional_improvement_log += f\"\\n- Original: {nfr['description']}\\n- Verifiable: {verifiable_nfr}\\n- Unambiguous: {unambiguous_nfr}\\n- Formatted: {formatted_nfr}\\n---\"\n",
    "    nfr[\"description\"] = formatted_nfr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the entire set of requirements for consistency and output as a JSON object\n",
    "consistency_improvement_log = \"\"\n",
    "consistent_req_json = json.loads(get_consistent_requirements(json.dumps(best_requirement_json, indent=4), USER_STORY))\n",
    "changes = difference(json.dumps(best_requirement_json), json.dumps(consistent_req_json))\n",
    "consistency_improvement_log = changes\n",
    "best_requirement_json = consistent_req_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the iteration content for review to refine future iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find next iteration increment for saving the output response\n",
    "import os\n",
    "\n",
    "i = 1\n",
    "while os.path.exists(f\"{OUTPUT_FILE_PREFIX.format(i=i)}\"):\n",
    "    i += 1\n",
    "\n",
    "output_filename = f\"{OUTPUT_FILE_PREFIX.format(i=i)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32635"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document the inputs into this set of generated requirements\n",
    "import pathlib\n",
    "\n",
    "# Add a placeholder for observations\n",
    "final_output = \"# Observations\\nPlaceholder for developer observations...\\n\\n\"\n",
    "\n",
    "# Capture the list of documents shared for RAG\n",
    "document_input_header = \"# Configuration\"\n",
    "document_list = \"\"\n",
    "for document in RAG_DOCUMENTS:\n",
    "    document_list += f\"{document}\\n\"\n",
    "final_output += f\"{document_input_header}\\n## RAG Files:\\n{document_list}\"\n",
    "final_output += f\"## Model Name\\n{MODEL_NAME}\\n\"\n",
    "final_output += f\"## Prompt\\n{USER_STORY}\"\n",
    "\n",
    "# Capture the system instructions prompt\n",
    "system_instructions_header = \"# System Instructions\"\n",
    "final_output += f\"\\n{system_instructions_header}\\n{SYSTEM_INSTRUCTIONS}\\n\"\n",
    "\n",
    "# Capture the intermediate changes made to improve requirement statements\n",
    "better_requirements_header = \"# Requirement Comparison Log\"\n",
    "final_output += f\"\\n{better_requirements_header}\\n{better_requirements_log}\"\n",
    "\n",
    "# Capture the intermediate changes for functional requirement improvements\n",
    "functional_improvement_header = \"# Functional Requirement Improvement Log\"\n",
    "final_output += f\"\\n{functional_improvement_header}\\n{functional_improvement_log}\"\n",
    "\n",
    "# Capture the intermediate changes for nonfunctional requirement improvements\n",
    "nonfunctional_improvement_header = \"# Non-Functional Requirement Improvement Log\"\n",
    "final_output += f\"\\n{nonfunctional_improvement_header}\\n{nonfunctional_improvement_log}\"\n",
    "\n",
    "# Capture the intermediate changes for consistency requirement improvements\n",
    "consistency_improvement_header = \"# Consistency Requirement Improvement Log\"\n",
    "final_output += f\"\\n{consistency_improvement_header}\\n{consistency_improvement_log}\"\n",
    "\n",
    "# Capture the final output message\n",
    "llm_output_header = \"# Final Output Message\"\n",
    "pretty_message = json.dumps(best_requirement_json, indent=4)\n",
    "final_output += f\"\\n{llm_output_header}\\n```json\\n{pretty_message}\\n```\"\n",
    "\n",
    "# Save the final message to a file\n",
    "pathlib.Path(output_filename).write_bytes(final_output.encode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a80b0b954e832a0f0eaa95d53ce03003001ae8d2a35e1423a1be8e7f67fa5ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
