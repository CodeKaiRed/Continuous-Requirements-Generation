{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Requirements Generation with GPT-4\n",
    "This notebook explores the use of utilizing GPT 4 on a domain-specific set of requirements. This model will be used downstream to manage requirements changes provided by external sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_NAME = \"gpt-4-turbo-preview\"\n",
    "\n",
    "IEEE_830 = \"data/IEEE 830-1998.pdf\"\n",
    "# BRIDGE_INSPECTION = \"data/a-state-of-the-art-review-of-bridge-inspection-planning-current-situation-and-future-needs.pdf\"\n",
    "SRS_PDF_PATH = \"data/2005 - pontis.pdf\"\n",
    "SRS_MD_PATH = \"data/output_srs.md\"\n",
    "SRS_TXT_PATH = \"data/output_srs.txt\"\n",
    "FINAL_OUTPUT_PATH = \"outputs/final_output.md\"\n",
    "\n",
    "JIRA_ISSUE = \"\"\"\n",
    "    As a highway information analyst, \n",
    "    I want to see traffic data in the map display\n",
    "    so that I can make better decisions for traffic impacts on bridges.\n",
    "\"\"\"\n",
    "\n",
    "# RAG_DOCUMENTS = [SRS_PDF_PATH, SRS_TXT_PATH, IEEE_830, BRIDGE_INSPECTION]\n",
    "RAG_DOCUMENTS = [SRS_PDF_PATH, SRS_MD_PATH, IEEE_830]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI api key\n",
    "import json\n",
    "\n",
    "config_data = json.load(open(\"config.json\"))\n",
    "openai_api_key = config_data[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the OpenAI Client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the system instructions from the prompt file\n",
    "with open(\"prompts/system_instructions.txt\", \"r\") as file:\n",
    "    system_instructions = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "The data format for fine-tuning GPT 3.5 must be in the conversational chat format seen below. The data will undergo cleaning and pre-processing to match the expected data format.\n",
    "```\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125977"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the SRS pdf file into a markdown file\n",
    "import pymupdf4llm\n",
    "import pathlib\n",
    "\n",
    "# Parse the pdf file into markdown format\n",
    "srs_md_text = pymupdf4llm.to_markdown(SRS_PDF_PATH)\n",
    "\n",
    "# Remove the repeating footers from the file\n",
    "srs_md_text = srs_md_text.replace(\"_Functional Requirements Specification v1.0_\", \"\")\n",
    "srs_md_text = srs_md_text.replace(\"_Pontis 5.0_\", \"\")\n",
    "\n",
    "# Save as a markdown file and text file for later usee\n",
    "pathlib.Path(SRS_MD_PATH).write_bytes(srs_md_text.encode())\n",
    "pathlib.Path(SRS_TXT_PATH).write_bytes(srs_md_text.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Promping GPT 3.5 Turbo for Requirements Change Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User prompt\n",
    "user_story_example = \"\"\"\n",
    "    As a user, \n",
    "    I want to add donation centers as favorites on my profile, \n",
    "    so that I can view them later.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9iWHPmXWXg52U0mNB9260y5cNkUd7', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"For the user story provided, we can break down the requirements into several artifacts as follows:\\n\\n### Functional Requirements:\\n\\n1. **FR1:** The system must allow users to add donation centers to a list of favorites.\\n   - **Location:** [output_srs.md: Functional Requirements Section]\\n\\n2. **FR2:** The system must provide users with the ability to view their list of favorited donation centers.\\n   - **Location:** [output_srs.md: Functional Requirements Section]\\n\\n### Non-Functional Requirements:\\n\\n1. **NFR1:** The system should update the user's list of favorites in real-time to ensure immediate access after a donation center is added.\\n   - **Location:** [output_srs.md: Non-Functional Requirements Section\", role='assistant', function_call=None, tool_calls=None))], created=1720397543, model='gpt-4-0125-preview', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=150, prompt_tokens=1023, total_tokens=1173))\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME, \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\" : \"system\",\n",
    "            \"content\": system_instructions\n",
    "        },\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\": user_story_example\n",
    "        }\n",
    "    ],\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 150,\n",
    "    top_p = 1\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Custom Assistant with File Search Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new assistant with access to the existing system requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Continuous Requirements Generator\",\n",
    "    instructions=system_instructions,\n",
    "    model=MODEL_NAME,\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the file and add them to a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=3, failed=0, in_progress=0, total=3)\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store caled \"Financial Statements\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"Gemini Software System Requirements\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = RAG_DOCUMENTS\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    " \n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the assistand to use the new Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_VxhElziE5mCs5myTkmgaZ140'])\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(SRS_TXT_PATH, \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": JIRA_ISSUE,\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "    }\n",
    "  ]\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a run and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user story provided by the highway information analyst desiring to see traffic data in the map display for making better traffic impact decisions on bridges, the following requirement artifacts have been prepared:\n",
      "\n",
      "### Functional Requirements\n",
      "\n",
      "1. **FR1: Display Traffic Data on Map**\n",
      "   - **Description:** The system shall allow highway information analysts to view traffic data overlaid on the map display.\n",
      "   - **Rationale:** To enable analysts to assess traffic impacts on bridge infrastructure visually.\n",
      "   - **Priority:** High\n",
      "   - **Location:** Section 4.0 Functional Requirements, after requirement 1.8 .\n",
      "\n",
      "### Use Case\n",
      "\n",
      "1. **Use Case: View Traffic Data on Map Display**\n",
      "   - **Goal:** Enable highway information analysts to make informed decisions regarding traffic impacts on bridges.\n",
      "   - **Actors:** Highway information analysts.\n",
      "   - **Preconditions:** The user is logged into the system with highway information analyst permissions.\n",
      "   - **Triggers:** The user selects the option to view traffic data on the map.\n",
      "   - **Basic Scenario:**\n",
      "     1. The user navigates to the map display feature.\n",
      "     2. The user selects the option to overlay traffic data.\n",
      "     3. The system retrieves current traffic data.\n",
      "     4. The system overlays traffic data on the map display.\n",
      "     5. The user analyzes the traffic data in relation to bridge locations and conditions.\n",
      "   - **Alternative Scenario:** If traffic data is unavailable, display a message indicating the lack of data.\n",
      "   - **Postconditions:** Traffic data is displayed on the map, enabling analysis.\n",
      "   - **Location:** Use Case section, after Use Case 4 .\n",
      "\n",
      "### Non-Functional Requirements\n",
      "\n",
      "1. **NFR1: Performance**\n",
      "   - **Description:** The system shall display traffic data on the map within 5 seconds of user request.\n",
      "   - **Rationale:** To ensure timely access to traffic information for decision-making.\n",
      "   - **Priority:** High\n",
      "   - **Location:** Section on Non-functional Requirements.\n",
      "\n",
      "2. **NFR2: Usability**\n",
      "   - **Description:** Traffic data overlay on the map must be easily interpretable and distinguishable from other map data.\n",
      "   - **Rationale:** To ensure that highway information analysts can easily understand and utilize the traffic data.\n",
      "   - **Priority:** Medium\n",
      "   - **Location:** Section on Non-functional Requirements.\n",
      "\n",
      "By placing these requirements in the specified sections of the SRS document, the system's ability to support highway information analysts in making better-informed decisions regarding traffic impacts on bridges will be effectively captured and addressed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8273"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document the inputs into this set of generated requirements\n",
    "\n",
    "# Add a placeholder for observations\n",
    "final_output = \"# Observations\\nPlaceholder for developer observations...\\n\\n\"\n",
    "\n",
    "# Capture the list of documents shared for RAG\n",
    "document_input_header = \"# Configuration\"\n",
    "document_list = \"\"\n",
    "for document in RAG_DOCUMENTS:\n",
    "    document_list += f\"{document}\\n\"\n",
    "final_output += f\"{document_input_header}\\n## RAG Files:\\n{document_list}\"\n",
    "final_output += f\"## Model Name\\n{MODEL_NAME}\\n\"\n",
    "final_output += f\"## Prompt{JIRA_ISSUE}\"\n",
    "\n",
    "# Capture the system instructions prompt\n",
    "system_instructions_header = \"# System Instructions\"\n",
    "final_output += f\"\\n{system_instructions_header}\\n{system_instructions}\\n\"\n",
    "\n",
    "# Capture the final output message\n",
    "llm_output_header = \"# Final Output Message\"\n",
    "final_output += f\"\\n{llm_output_header}\\n{message_content.value}\\n\"\n",
    "\n",
    "# Save the final message to a file\n",
    "pathlib.Path(FINAL_OUTPUT_PATH).write_bytes(final_output.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing_extensions import override\n",
    "# from openai import AssistantEventHandler, OpenAI\n",
    " \n",
    "# client = OpenAI(api_key=openai_api_key)\n",
    " \n",
    "# class EventHandler(AssistantEventHandler):\n",
    "#     @override\n",
    "#     def on_text_created(self, text) -> None:\n",
    "#         print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "\n",
    "#     @override\n",
    "#     def on_tool_call_created(self, tool_call):\n",
    "#         print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "\n",
    "#     @override\n",
    "#     def on_message_done(self, message) -> None:\n",
    "#         # print a citation to the file searched\n",
    "#         message_content = message.content[0].text\n",
    "#         annotations = message_content.annotations\n",
    "#         citations = []\n",
    "#         for index, annotation in enumerate(annotations):\n",
    "#             message_content.value = message_content.value.replace(\n",
    "#                 annotation.text, f\"[{index}]\"\n",
    "#             )\n",
    "#             if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "#                 cited_file = client.files.retrieve(file_citation.file_id)\n",
    "#                 citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "#         print(message_content.value)\n",
    "#         print(\"\\n\".join(citations))\n",
    "\n",
    "\n",
    "# # Then, we use the stream SDK helper\n",
    "# # with the EventHandler class to create the Run\n",
    "# # and stream the response.\n",
    "\n",
    "# with client.beta.threads.runs.stream(\n",
    "#     thread_id=thread.id,\n",
    "#     assistant_id=assistant.id,\n",
    "#     instructions=system_instructions,\n",
    "#     event_handler=EventHandler(),\n",
    "# ) as stream:\n",
    "#     stream.until_done()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a80b0b954e832a0f0eaa95d53ce03003001ae8d2a35e1423a1be8e7f67fa5ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
